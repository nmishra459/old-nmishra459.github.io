<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
	<meta content='no-cache' http-equiv='cache-control'>
	<meta content='0' http-equiv='expires'>
	<meta content='no-cache' http-equiv='pragma'>
	<meta content="Nishant's Personal Website" name="title" property="og:title">
	<meta content="assets/site_screenshot.JPG" name="image" property="og:image">
	<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" rel="stylesheet">
	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js">
	</script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js">
	</script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js">
	</script>
	<link href="https://cdn.jsdelivr.net/gh/gitbrent/bootstrap4-toggle@3.6.1/css/bootstrap4-toggle.min.css" rel="stylesheet">
	<script src="https://cdn.jsdelivr.net/gh/gitbrent/bootstrap4-toggle@3.6.1/js/bootstrap4-toggle.min.js">
	</script>
	<script>
	           function updateSite() {
	               location.replace("research.html");
	           }

	</script>
	<link href="//fonts.googleapis.com/css?family=Inconsolata" rel="stylesheet" type="text/css">
	<link href='https://fonts.googleapis.com/css?family=Oxygen' rel='stylesheet'>
	<link href="assets/Nishant_CloseUp_Dark.jpg" rel="icon" sizes="16x16" type="image">
	<link href="styles-dark.css" rel="stylesheet">
	<title>Nishant Mishra | Research</title>
</head>
<body style="background-color: #0d1017;">
	<ul class="nav sticky-top nav-pills" style="background-color: #161b21;">
		<li class="nav-item" style="padding-top:7px; padding-bottom:5px;">
			<a class="nav-link" href="index-dark.html">Home</a>
		</li>
		<li class="nav-item" style="padding-top:7px; padding-bottom:5px;">
			<a class="nav-link active" href="research-dark.html">Research</a>
		</li>
		<li class="nav-item" style="padding-top:7px; padding-bottom:5px;">
			<a class="nav-link" href="personal-dark.html">Personal</a>
		</li>
		<li class="nav-item" style="padding-top:7px; padding-bottom:5px;">
			<a class="nav-link" href="courses-dark.html">Coursework</a>
		</li>
		<li class="nav-item" style="padding-left:15px; padding-top:7px; padding-bottom:5px;"><input data-off="🌙" data-offstyle="outline-secondary" data-on="☀️" data-onstyle="info" data-style="quick" data-toggle="toggle" id="toggle-event" onchange="updateSite()" type="checkbox"></li>
	</ul><br>


	<h2 align="center"><u><strong>Research Projects</strong></u></h2><br>
	<div class="container">
		<div class="row">
			<div align="center" class="col" id='personal'>
				<h2>Machine Learning Algorithms for Parameterizing Black Hole Images</h2>
				<p>In 2019, the Event Horizon Telescope Collaboration (<a href="https://eventhorizontelescope.org/" id="IACS" type="link"><b>EHT</b></a>) captured the first direct images of a black hole, serving as a major milestone in astrophysical classification. As a machine learning research intern at the Harvard Institute for Applied Computational Science (<a href="https://iacs.seas.harvard.edu/" id="Princeton" type="link"><b>StellarDNN Group, IACS</b></a>), I use this data to run sections of the analysis pipeline for a machine learning recognition pathway (<b>Python, PyTorch, Keras</b>), exploring ways to not only pull physical parameters from images like the ones from EHT, but also create higher-resolution simulated images of what these black holes images might look like had we had even more powerful telescopes.<br></p>
			</div>
			<div class="col text-right"><br><br><img alt="Responsive image" class="img-fluid border border-dark" src="assets/IACS.png"></div>
		</div>
	</div>

	<hr style="border-color:#161b21;">

	<br>
	<div class="container">
		<div class="row">
			<div class="col"><br><img alt="Responsive image" class="img-fluid border border-dark" src="assets/PuchallaLab.png"></div>
			<div align="center" class="col right text-align left" id='personal'>
				<h2>Deep Learning Algorithms for Following Zebrafish Growth</h2>
				<p>Zebrafish are great test subjects for medical research! But they all look similar, and they grow quickly, which makes it difficult for researchers to accurately identify them for trials. Working as a machine learning research intern with my colleagues at Princeton University (<a href="https://sites.google.com/view/puchallalab/home?authuser=0" id="Princeton" type="link"><b>Puchalla Lab, Dept. of Physics</b></a>), I ran sections of the analysis pipeline for a deep learning recognition pathway (based off Google's Inception V3 model) that uses a temporal bootstrapping technique to identify individual zebrafish over several weeks (<b>Python, Tensorflow, Keras</b>). Our goal is to offer an alternative to more labor-intensive and invasive identification methods such as subdermal dye-injection and RFID tag implants. [publication in progress]<br></p>
			</div>
		</div>
	</div>
	<br>

	<hr style="border-color:#161b21;">

	<br>
	<div class="container">
		<div class="row">
			<div align="center" class="col" id='personal'>
				<br>
				<h2>Modeling the Statistical Mechanics of Self-Gravitating Systems</h2>
				<p>In 2020, I worked as a computational physics research intern at the Princeton Plasma Physics Laboratory (<a href="https://www.pppl.gov/" id="PPPL" type="link"><b>PPPL</b></a>). As a part of PPPL's <a href = "https://theory.pppl.gov/research/research.php?rid=3" id = "PPPL" type = "link"><b>Computational Plasma Physics Group</b></a> (Theory Department), I focused on mapping out the statistical mechanics of self-gravitating systems as well as comparing them to those of plasma-based systems. First, I developed an adaptive symplectic integrator <b>(C++, Python)</b> that can model the complex motion - including binary captures - of dozens of closely distributed particles in space with reasonable accuracy. I then utilized these algorithms to study and record the underlying energy patterns and discrepancies in these systems. At the end of Summer 2020, I presented my work at PPPL's Summer Internship Poster Session.<br>
				<br>
				<a class="btn btn-outline-primary" href="assets/An Adaptive Symplectic Integrator for Modeling the Mechanics of Self-Gravitating Systems.pdf"><b>Research Poster</b></a> <a class="btn btn-outline-primary" href="assets/PPPL Presentation.pdf"><b>Research Presentation</b></a></p>
			</div>
			<div class="col text-right">
				<video autoplay muted loop class="embed-responsive embed-responsive-1by1" height="440" width="540"><source class="video-mask" src="assets/animation_fast.mp4" type="video/mp4"> Your browser does not support the video tag.</video>
			</div>
		</div>
	</div>
	<br>

	<hr style="border-color:#161b21;">

	<br>
	<div class="container">
		<div class="row">
			<div class="col text-right"><img alt="Responsive image" class="img-fluid border border-dark" src="assets/DepthMap.png" height="470" width="470"></div>
			<div align="center" class="col right text-align left" id='personal'>
				<br><br>
				<h2>Deep Learning Algorithms for Classifying Underwater Pollution</h2>
				<p>Since the late 1960s, underwater pollution has become an increasingly worse problem in the earth’s oceans. To help classify some of the different types of ocean pollution that one might encounter, I worked at Princeton University (<a href="https://sites.google.com/view/puchallalab/home?authuser=0" id="Princeton" type="link"><b>Puchalla Lab, Dept. of Physics</b></a>) to capture 3D texture maps of common trash such as cups, cans, and bottles to serve as data for a deep learning recognition pathway (<b>Python, Tensorflow, Keras</b>). After running these depth maps through the classification pipeline, I compared the results to the classification done if the pipeline was just given standard RGB images. We hope that continued studies in this field will lead to better regional optimization of oceanic clean-up efforts.<br></p>
				<br>
			</div>
		</div>
	</div>
	<br>

	<hr style="border-color:#161b21;">

	<br>
	<div class="container">
		<div class="row">
			<div align="center" class="col" id='personal'>
				<br>
				<h2>A Microfluidic Flow System for Studying Single-Particle Kinetics</h2>
				<p>In the summer of 2019, I worked as a microfluidics r&d engineering intern through Princeton University's Laboratory Learning Program (<a href="https://research.princeton.edu/about-us/internships/laboratory-learning-program" id="LLP" type="link"><b>LLP</b></a>). As a part of <a href="https://sites.google.com/view/puchallalab/home?authuser=0" id="Princeton" type="link"><b>Puchalla Lab</b></a>, I set up a multi-channel syringe pump system that facilitates single-particle kinetics in PDMS microchannels. To enable users to set, change, and read flow rates/directions, I programmed a user interface with <b>LabVIEW</b>, a graphical programming environment. I also worked on a set of <b>MATLAB</b> scripts that simulate the distribution of fluorescence bursts under various testing conditions.<br>
				<br>
				<a class="btn btn-outline-primary" href="assets/A Multi-Channel Microfluidic Flow System for Facilitating and Studying Single-Particle Kinetics.pdf"><b>Research Poster</b></a></p>
			</div>
			<div class="col text-right"><img alt="Responsive image" class="img-fluid border border-dark" src="assets/Microfluidics.png"></div>
		</div>
	</div>
	<br>

</body>
</html>
